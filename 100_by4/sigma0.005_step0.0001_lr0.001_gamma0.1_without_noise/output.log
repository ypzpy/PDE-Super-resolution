2024-02-21 22:07:23,613 : Training for 1000 epoches and [GP_l, GP_sigma, ll_sigma, step_size, learning_rate] : [0.1, 0.1, 0.005, 0.0001, 0.001]
2024-02-21 22:07:29,709 : Epoch 1/1000: Loss = 32.26958465576172
2024-02-21 22:07:35,327 : Epoch 2/1000: Loss = 47.85718536376953
2024-02-21 22:07:41,082 : Epoch 3/1000: Loss = 39.47815704345703
2024-02-21 22:07:47,011 : Epoch 4/1000: Loss = 38.204490661621094
2024-02-21 22:07:53,065 : Epoch 5/1000: Loss = 23.478464126586914
2024-02-21 22:07:59,103 : Epoch 6/1000: Loss = 46.33537673950195
2024-02-21 22:08:05,319 : Epoch 7/1000: Loss = 37.12727737426758
2024-02-21 22:08:11,390 : Epoch 8/1000: Loss = 28.308202743530273
2024-02-21 22:08:16,541 : Epoch 9/1000: Loss = 29.628543853759766
2024-02-21 22:08:21,613 : Epoch 10/1000: Loss = 39.162391662597656
2024-02-21 22:08:27,564 : Epoch 11/1000: Loss = 29.357051849365234
2024-02-21 22:08:33,351 : Epoch 12/1000: Loss = 64.81987762451172
2024-02-21 22:08:39,176 : Epoch 13/1000: Loss = 47.652748107910156
2024-02-21 22:08:45,587 : Epoch 14/1000: Loss = 35.09098815917969
2024-02-21 22:08:51,061 : Epoch 15/1000: Loss = 38.42839050292969
2024-02-21 22:08:56,161 : Epoch 16/1000: Loss = 29.062744140625
2024-02-21 22:09:01,606 : Epoch 17/1000: Loss = 19.509037017822266
2024-02-21 22:09:06,951 : Epoch 18/1000: Loss = 44.31673812866211
2024-02-21 22:09:11,492 : Epoch 19/1000: Loss = 28.760398864746094
2024-02-21 22:09:15,691 : Epoch 20/1000: Loss = 31.414548873901367
2024-02-21 22:09:21,834 : Epoch 21/1000: Loss = 40.130985260009766
2024-02-21 22:09:28,214 : Epoch 22/1000: Loss = 33.86692810058594
2024-02-21 22:09:33,913 : Epoch 23/1000: Loss = 29.07328987121582
2024-02-21 22:09:39,286 : Epoch 24/1000: Loss = 36.41641616821289
2024-02-21 22:09:44,469 : Epoch 25/1000: Loss = 43.10887145996094
2024-02-21 22:09:50,177 : Epoch 26/1000: Loss = 52.099853515625
2024-02-21 22:09:54,844 : Epoch 27/1000: Loss = 32.42256164550781
