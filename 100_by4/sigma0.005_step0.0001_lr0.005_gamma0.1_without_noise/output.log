2024-02-21 22:03:00,730 : Training for 1000 epoches and [GP_l, GP_sigma, ll_sigma, step_size, learning_rate] : [0.1, 0.1, 0.005, 0.0001, 0.005]
2024-02-21 22:03:06,787 : Epoch 1/1000: Loss = 35.97080993652344
2024-02-21 22:03:12,691 : Epoch 2/1000: Loss = 50.338260650634766
2024-02-21 22:03:20,041 : Epoch 3/1000: Loss = 57.81155776977539
2024-02-21 22:03:26,371 : Epoch 4/1000: Loss = 58.37764358520508
2024-02-21 22:03:31,869 : Epoch 5/1000: Loss = 54.164306640625
2024-02-21 22:03:37,161 : Epoch 6/1000: Loss = 69.94857025146484
2024-02-21 22:03:42,052 : Epoch 7/1000: Loss = 48.59885787963867
2024-02-21 22:03:46,794 : Epoch 8/1000: Loss = 55.84844207763672
2024-02-21 22:03:52,417 : Epoch 9/1000: Loss = 50.20649337768555
2024-02-21 22:03:58,071 : Epoch 10/1000: Loss = 56.370269775390625
2024-02-21 22:04:03,814 : Epoch 11/1000: Loss = 50.38134002685547
2024-02-21 22:04:10,003 : Epoch 12/1000: Loss = 40.764427185058594
2024-02-21 22:04:15,889 : Epoch 13/1000: Loss = 38.08491516113281
2024-02-21 22:04:22,885 : Epoch 14/1000: Loss = 48.75975036621094
2024-02-21 22:04:29,559 : Epoch 15/1000: Loss = 52.96579360961914
2024-02-21 22:04:34,549 : Epoch 16/1000: Loss = 52.54192352294922
2024-02-21 22:04:39,266 : Epoch 17/1000: Loss = 53.82584762573242
2024-02-21 22:04:44,786 : Epoch 18/1000: Loss = 61.055233001708984
2024-02-21 22:04:49,724 : Epoch 19/1000: Loss = 50.1041259765625
2024-02-21 22:04:55,542 : Epoch 20/1000: Loss = 37.972564697265625
2024-02-21 22:05:01,593 : Epoch 21/1000: Loss = 53.458473205566406
2024-02-21 22:05:06,866 : Epoch 22/1000: Loss = 50.095481872558594
2024-02-21 22:05:13,292 : Epoch 23/1000: Loss = 42.80281066894531
2024-02-21 22:05:19,117 : Epoch 24/1000: Loss = 46.51996994018555
