2024-02-01 16:35:05,061 : Training for 1000 epoches and [GP_l, GP_sigma, ll_sigma, step_size, learning_rate] : [0.1, 0.1, 0.005, 0.0001, 0.001]
2024-02-01 16:35:17,064 : Epoch 1/1000: Loss = 89960544.0
2024-02-01 16:35:26,023 : Epoch 2/1000: Loss = 80181568.0
2024-02-01 16:35:34,977 : Epoch 3/1000: Loss = 70567968.0
2024-02-01 16:35:43,785 : Epoch 4/1000: Loss = 43694400.0
2024-02-01 16:35:52,504 : Epoch 5/1000: Loss = 30116220.0
2024-02-01 16:36:01,322 : Epoch 6/1000: Loss = 21965392.0
2024-02-01 16:36:10,318 : Epoch 7/1000: Loss = 12042266.0
2024-02-01 16:36:19,197 : Epoch 8/1000: Loss = 8800758.0
2024-02-01 16:36:27,958 : Epoch 9/1000: Loss = 6616509.0
2024-02-01 16:36:36,744 : Epoch 10/1000: Loss = 4825366.0
2024-02-01 16:36:45,572 : Epoch 11/1000: Loss = 4191080.25
2024-02-01 16:36:54,696 : Epoch 12/1000: Loss = 4054233.5
2024-02-01 16:37:03,811 : Epoch 13/1000: Loss = 3772017.25
2024-02-01 16:37:12,673 : Epoch 14/1000: Loss = 3531063.5
2024-02-01 16:37:21,697 : Epoch 15/1000: Loss = 3232540.0
2024-02-01 16:37:30,840 : Epoch 16/1000: Loss = 2931954.0
2024-02-01 16:37:39,881 : Epoch 17/1000: Loss = 2738727.0
2024-02-01 16:37:48,715 : Epoch 18/1000: Loss = 2737708.25
2024-02-01 16:37:57,726 : Epoch 19/1000: Loss = 2960023.0
2024-02-01 16:38:06,848 : Epoch 20/1000: Loss = 2506201.5
2024-02-01 16:38:15,846 : Epoch 21/1000: Loss = 2313237.0
2024-02-01 16:38:24,927 : Epoch 22/1000: Loss = 2341401.75
2024-02-01 16:38:33,839 : Epoch 23/1000: Loss = 2199436.25
2024-02-01 16:38:42,826 : Epoch 24/1000: Loss = 2094453.625
2024-02-01 16:38:51,817 : Epoch 25/1000: Loss = 2099254.5
2024-02-01 16:39:00,903 : Epoch 26/1000: Loss = 2109274.25
2024-02-01 16:39:09,844 : Epoch 27/1000: Loss = 2165181.75
2024-02-01 16:39:18,790 : Epoch 28/1000: Loss = 1942925.25
2024-02-01 16:39:28,025 : Epoch 29/1000: Loss = 1915994.25
2024-02-01 16:39:37,275 : Epoch 30/1000: Loss = 1770930.625
2024-02-01 16:39:46,402 : Epoch 31/1000: Loss = 1732414.25
2024-02-01 16:39:55,608 : Epoch 32/1000: Loss = 1781754.25
2024-02-01 16:40:04,616 : Epoch 33/1000: Loss = 1717965.0
2024-02-01 16:40:13,800 : Epoch 34/1000: Loss = 1725396.25
2024-02-01 16:40:22,908 : Epoch 35/1000: Loss = 1636822.5
2024-02-01 16:40:32,088 : Epoch 36/1000: Loss = 1682028.5
2024-02-01 16:40:41,085 : Epoch 37/1000: Loss = 1655877.25
2024-02-01 16:40:50,190 : Epoch 38/1000: Loss = 1507978.875
2024-02-01 16:40:59,227 : Epoch 39/1000: Loss = 1511672.25
2024-02-01 16:41:08,151 : Epoch 40/1000: Loss = 1624877.0
2024-02-01 16:41:16,982 : Epoch 41/1000: Loss = 1572646.375
2024-02-01 16:41:25,838 : Epoch 42/1000: Loss = 1421552.75
2024-02-01 16:41:34,750 : Epoch 43/1000: Loss = 1562174.0
2024-02-01 16:41:43,823 : Epoch 44/1000: Loss = 1355629.0
2024-02-01 16:41:52,786 : Epoch 45/1000: Loss = 1438368.75
2024-02-01 16:42:01,764 : Epoch 46/1000: Loss = 1367999.0
2024-02-01 16:42:10,669 : Epoch 47/1000: Loss = 1445532.25
2024-02-01 16:42:19,685 : Epoch 48/1000: Loss = 1243571.0
2024-02-01 16:42:28,775 : Epoch 49/1000: Loss = 1593043.25
2024-02-01 16:42:37,829 : Epoch 50/1000: Loss = 3954600.75
2024-02-01 16:42:46,751 : Epoch 51/1000: Loss = 1535440.875
2024-02-01 16:42:55,635 : Epoch 52/1000: Loss = 1592548.25
2024-02-01 16:43:04,622 : Epoch 53/1000: Loss = 1726583.25
2024-02-01 16:43:13,688 : Epoch 54/1000: Loss = 1601375.0
